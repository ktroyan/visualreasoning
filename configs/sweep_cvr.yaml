# NOTE: The parameters structure should follow that of the nested keys used in the config files in /configs by using "parameters" to create imbrication

# Program to run
program: experiment.py

# Method to use for HPs optimization: random, grid, bayes
method: grid

# Project of the sweep
# project: 'VisReas-project'
# entity: 'klim-t'

# Metric to optimize
metric:
  goal: minimize
  name: metrics/val_loss_epoch  # TODO: see why metrics/val_loss does not exist in the Sweep section on WandB

##### Experiment sweep #####
parameters:
  experiment:
    parameters:
      study:
        values: ['sample-efficiency']
        # values: ['compositionality']
        # values: ['sys-gen']
      setting:
        values: ['exp_setting_1'] # sample-efficiency
        # values: ['exp_setting_1', 'exp_setting_2', 'exp_setting_3']  # compositionality
        # values: ['exp_setting_1']  # sys-gen
      name:
        values: ['experiment_1', 'experiment_2', 'experiment_3', 'experiment_4', 'experiment_5', 'experiment_6']  # sample-efficiency
        # values: ['experiment_1', experiment_2', 'experiment_3', 'experiment_4', 'experiment_5', 'experiment_6', 'experiment_7', 'experiment_8', 'experiment_9', 'experiment_10']  # compositionality; the number of experiments per setting is not the same across settings
        # values: ['experiment_1', 'experiment_2']  # sys-gen

##### Hyperparameters sweep #####
# parameters:
#   model:  # model config
#     parameters:
#       training_hparams:
#         parameters:
#           optimizer:
#             values: ['Adam', 'SGD']
#           scheduler:
#             parameters:
#               type:
#                 values: ['CosineAnnealingLR', 'ReduceLROnPlateau', 'StepLR']
#           lr_warmup:
#             parameters:
#               enabled:
#                 values: [true, false]
#               num_steps:
#                 values: [100.0]
#           lr:
#             values: [0.0001, 0.0005, 0.001]
#           wd:
#             values: [0.001]
#       backbone:
#         values: ['transformer', 'vit', 'vit_timm', 'resnet']
#       head:
#         values: ['mlp']
#       patch_size:
#         values: [8, 16]
#       ape:
#         parameters:
#           enabled:
#             values: [true, false]
#           ape_type:
#             values: ['learn', '2dsincos']  # absolute positional encoding
#       use_cls_token:
#         values: [true]  # has to be true, otherwise does not work
#       num_reg_tokens:
#         values: [0, 4, 8]
#       encoder_aggregation:
#         parameters:
#           enabled:
#             values: [true, false]
#           method:
#             values: ['mean', 'max', 'token']
#       task_embedding:
#         parameters:
#           enabled:
#             values: [true, false]
#           task_embedding_dim:
#             values: [128]
#       dp_sim:
#         parameters:
#           enabled:
#             values: [true, false]
#   backbone_network: # backbone network config
#     parameters:
#       embed_dim:
#         values: [64, 256]
#       num_heads:
#         values: [4]
#       num_layers:
#         values: [4, 12]
#   data:
#     parameters:
#       train_batch_size:
#         values: [64, 128]
# early_terminate:
#   type: hyperband
#   s: 2
#   eta: 3
#   max_iter: 10