# NOTE: The parameters structure should follow that of the nested keys used in the config files in /configs by using "parameters" to create imbrication

# Program to run
program: experiment.py

# Method to use for HPs optimization: random, grid, bayes
method: grid

# Project of the sweep
# project: 'VisReas-project'
# entity: 'klim-t'

# Metric to optimize
metric:
  goal: minimize
  name: metrics/val_loss_epoch  # TODO: see why metrics/val_loss does not exist in the Sweep section on WandB

# Hyperparameters space to search
parameters:
  model:  # model config
    parameters:
      training_hparams:
        parameters:
          optimizer:
            values: ['Adam', 'SGD']
          scheduler:
            values: ['CosineAnnealingLR', 'ReduceLROnPlateau', 'StepLR']
          lr_warmup:
            parameters:
              enabled:
                values: [true, false]
              num_steps:
                values: [100.0]
          lr:
            values: [0.0001, 0.0005, 0.001]
          wd:
            values: [0.001]
      backbone:
        values: ['transformer', 'vit', 'vit_timm', 'resnet']
      head:
        values: ['mlp']
      patch_size:
        values: [8, 16]
      ape_type:
        values: ['2dsincos', 'learn']
      use_cls_token:
        values: [true]  # has to be true, otherwise does not work
      num_reg_tokens:
        values: [0, 4, 8]
      encoder_aggregation:
        parameters:
          enabled:
            values: [true, false]
          method:
            values: ['mean', 'max', 'token']
      task_embedding:
        parameters:
          enabled:
            values: [true, false]
          task_embedding_dim:
            values: [128]
      dp_sim:
        parameters:
          enabled:
            values: [true, false]
  backbone_network: # backbone network config
    parameters:
      embed_dim:
        values: [64, 256]
      num_heads:
        values: [4]
      num_layers:
        values: [4, 12]
  data:
    parameters:
      train_batch_size:
        values: [64, 128]
# early_terminate:
#   type: hyperband
#   s: 2
#   eta: 3
#   max_iter: 10