model:
  backbone: 'looped_vit'  # choices: 'vit', 'looped_vit', 'transformer', 'resnet'
  head: 'mlp'  # choices: 'transformer', 'mlp', 'mytransformer', 'xtransformer'
  pretrained: null
  patch_size: 1
  visual_tokens:
    enabled: true
  use_ohe_repr: true # if True, use OHE for the possible tokens thus yielding as many artificial channels as there are different possible tokens; if False, use a simple sequence of tokens with an artificial channel only created to use convolution for linear projection of patches/pixels/tokens
  ape:
    enabled: true
    ape_type: '2dsincos'  # choices: 'learn', '2dsincos'; absolute positional encoding
    mixer: 'sum'  # choices: 'sum', 'weighted_sum', weighted_sum_vec, ...; how to mix the input embeddings and the positional embeddings
  ope:
    enabled: false
  rpe:
    enabled: true
    rpe_type: 'rope'  # choices: 'rope', 'Two-slope-Alibi', 'Four-diag-slope-Alibi'; relative positional encoding
  pondernet:
    enabled: ${resolve_pondernet_for_looped_vit:${model.backbone}, true}  # whether to use PonderNet or not; false for bakcbones other than looped_vit
    max_steps: 15   # maximum number of steps; use higher number for more complex tasks to cover more of the geometric distribution mass
    epsilon: 0.05   # to stop when the cumulative probability mass is greater than epsilon (i.e., when the remaining probability mass is smaller than epsilon)
    hidden_dim: 128 # hidden dimension for the haltig node
    beta: 0.01      # KLD regularization coefficient; how much do we force the geometric distribution prior
    lambda_p: 0.1   # for 0.5, the expected value for the number of steps/iterations is 2, for 0.1 it is 10; for example: 0.5 for simple tasks, 0.1 for more complex tasks
  use_cls_token: false # whether to use a cls token or not
  num_reg_tokens: 0 # number of register tokens to prepend to the input sequence
  encoder_aggregation:
    enabled: false
    method: '' # choices: '' (for no pooling and returning a sequence), 'mean', 'max', 'token' (for aggregating features and returning the cls token)
  num_elementary_tasks: 14    # number of elementary transformations + 1 for identity as padding; used for the task embedding; NOTE: this should match the transformations in the data module
  task_embedding:
    enabled: ${resolve_if_then_else_compositionality:${experiment.study}, true}  # if the study is compositionality, then whether the task embedding is used or not depends on the set value. Otherwise, it is not used.
    approach: ${resolve_if_then_else:${model.task_embedding.enabled}, task_tokens}  # choices: task_tokens, example_in_context
  training_hparams:
    optimizer: 'AdamW' # choices: Adam, AdamW, SGD
    scheduler:
      monitored_metric: ${resolve_if_then_else_validate_in_and_out_domain:${data.validate_in_and_out_domain}, gen_val_loss, val_loss} # choices: 'gen_val_loss', 'gen_val_acc', 'val_acc', 'val_loss'
      type: 'CosineAnnealingLR' # choices: ReduceLROnPlateau, StepLR, CosineAnnealingLR
      interval: 'epoch' # choices: 'step', 'epoch'
      frequency: 1
    lr_warmup:
      enabled: true  # use true for Transformer-based models
      type: 'linear' # choices: 'linear'
      num_steps: 200.0
    lr: 0.001  # 0.001
    wd: 0.001  # 0.001
  attention_map:
    enabled: false
    layer: -1  # layer to use for the attention map (-1 for the last, 0 for the first layer, 1 for the second layer, etc.)
    n_samples: 2  # number of samples to consider for the attention map
  observe_preds:
    enabled: true
    n_samples: 4
