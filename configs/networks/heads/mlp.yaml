head_network:
  name: 'mlp'
  # n_layers: 2
  hidden_dim: 128
  activation: 'relu'