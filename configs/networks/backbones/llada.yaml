backbone_network:
  name: 'llada'
  activation_type: silu
  attention_dropout: 0.0
  attention_layer_norm: false
  attention_layer_norm_with_affine: true
  bias_for_layer_norm: false
  block_group_size: 1
  block_type: llama
  embed_dim: 128
  embedding_dropout: 0.0
  embedding_size: 15
  eos_token_id: null
  flash_attention: false
  include_bias: false
  include_qkv_bias: false
  init_cutoff_factor: null
  init_device: cuda
  init_fn: mitchell
  init_std: 0.02
  input_emb_norm: false
  layer_norm_type: rms
  layer_norm_with_affine: true
  mask_token_id: 15
  max_sequence_length: 4096
  mlp_hidden_size: 384
  mlp_ratio: 4
  model_type: llada
  multi_query_attention: null
  n_heads: 4
  n_kv_heads: 4
  n_layers: 6
  pad_token_id: 10
  precision: amp_bf16
  residual_dropout: 0.0
  rms_norm_eps: 1e-05
  rope: true
  rope_full_precision: true
  rope_theta: 500000.0
  scale_logits: false
  use_cache: false
  vocab_size: 15
  weight_tying: false
  diffusion:
    steps: 32
    cfg_scale: 0.0
    temperature: 0.0
    remasking: low_confidence
    sage_thinking: false

