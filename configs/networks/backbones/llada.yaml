backbone_network:
  name: 'llada'
  activation_type: silu
  alibi: false
  alibi_bias_max: 8.0
  attention_dropout: 0.0
  attention_layer_norm: false
  attention_layer_norm_with_affine: true
  bias_for_layer_norm: false
  block_group_size: 1
  block_type: llama
  d_model: 384 # 4096
  embedding_dropout: 0.0
  embedding_size: 126464
  eos_token_id: 126081
  flash_attention: false
  include_bias: false
  include_qkv_bias: false
  init_cutoff_factor: null
  init_device: cuda
  init_fn: mitchell
  init_std: 0.02
  input_emb_norm: false
  layer_norm_type: rms
  layer_norm_with_affine: true
  mask_token_id: 126336
  max_sequence_length: 4096
  mlp_hidden_size: 1152
  mlp_ratio: 4
  model_type: llada
  multi_query_attention: null
  n_heads: 8 # 32
  n_kv_heads: 8 # 32
  n_layers: 8 # 32
  pad_token_id: 126081
  precision: amp_bf16
  residual_dropout: 0.0
  rms_norm_eps: 1e-05
  rope: true
  rope_full_precision: true
  rope_theta: 500000.0
  scale_logits: false
  use_cache: false
  vocab_size: 126464
  weight_tying: false
  diffusion:
    steps: 8
    cfg_scale: 0.0
    temperature: 0.0
    remasking: low_confidence
    sage_thinking: false

